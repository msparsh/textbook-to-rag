In general, a recursive definition is made up of two parts. There is at least one base case that directly specifies the result for a special case (case 1 in the example above), and there is at least one recursive (inductive) case (case 2 in the example above) that defines the answer in terms of the answer to the question on some other input, typically a simpler version of the same problem. It is the presence of a base case that keeps a recursive definition from being a circular definition.39 The world's simplest recursive definition is probably the factorial function (typically written in mathematics using !) on natural numbers.40 The classic inductive definition is The first equation defines the base case. The second equation defines factorial for all natural numbers, except the base case, in terms of the factorial of the previous number. Figure 6-1 contains both an iterative ( fact_iter) and a recursive ( fact_rec) implementation of factorial. Figure 6-1 Iterative and recursive implementations of factorial Figure 6-1 Iterative and recursive implementations of factorial This function is sufficiently simple that neither implementation is hard to follow. Still, the second is a more direct translation of the original recursive definition. It almost seems like cheating to implement fact_rec by calling fact_rec from within the body of fact_rec. It works for the same reason that the iterative implementation works. We know that the iteration in fact_iter will terminate because n starts out positive and each time around the loop it is reduced by 1. This means that it cannot be greater than 1 forever.

Similarly, if fact_rec is called with 1, it returns a value without making a recursive call. When it does make a recursive call, it always does so with a value one less than the value with which it was called. Eventually, the recursion terminates with the call fact_rec(1). Finger exercise: The harmonic sum of an integer, n > 0, can be calculated using the formula . Write a recursive function that computes this. 6.1 Fibonacci Numbers The Fibonacci sequence is another common mathematical function that is usually defined recursively. “They breed like rabbits,” is often used to describe a population that the speaker thinks is growing too quickly. In the year 1202, the Italian mathematician Leonardo of Pisa, also known as Fibonacci, developed a formula to quantify this notion, albeit with some not terribly realistic assumptions.41 Suppose a newly born pair of rabbits, one male and one female, are put in a pen (or worse, released in the wild). Suppose further that the rabbits can mate at the age of one month (which, astonishingly, some breeds can) and have a one-month gestation period (which, astonishingly, some breeds do). Finally, suppose that these mythical rabbits never die (not a property of any known breed of rabbit), and that the female always produces one new pair (one male, one female) every month from its second month on. How many female rabbits will there be at the end of six months? On the last day of the first month (call it month 0), there will be one female (ready to conceive on the first day of the next month).

For each recursive call, the value of the decrementing function is less than the value of the decrementing function on entry to the instance of the function making the call. The decrementing function for bin_search is high–low. The if statement in search ensures that the value of this decrementing function is at least 0 the first time bSearch is called (decrementing function property 1). When bin_search is entered, if high–low is exactly 0, the function makes no recursive call—simply returning the value L[low] == e (satisfying decrementing function property 2). The function bin_search contains two recursive calls. One call uses arguments that cover all the elements to the left of mid, and the other call uses arguments that cover all the elements to the right of mid. In either case, the value of high–low is cut in half (satisfying decrementing function property 3). We now understand why the recursion terminates. The next question is how many times can the value of high–low be cut in half before high–low == 0? Recall that logy(x) is the number of times that y has to be multiplied by itself to reach x. Conversely, if x is divided by y logy(x) times, the result is 1. This implies that high–low can be cut in half using floor division at most log2(high–low) times before it reaches 0. Finally, we can answer the question, what is the algorithmic complexity of binary search? Since when search calls bSearch the value of high–low is equal to len(L)-1, the complexity of search is θ(log(len(L))).73 Finger exercise: Why does the code use mid+1 rather than mid in the second recursive call?

15.1 Fibonacci Sequences, Revisited In Chapter 4, we looked at a straightforward recursive implementation of the Fibonacci function: def fib(n): """Assumes n is an int >= 0 Returns Fibonacci of n""" if n == 0 or n == 1: return 1 else: return fib(n-1) + fib(n-2) While this implementation of the recurrence is obviously correct, it is terribly inefficient. Try, for example, running fib(120), but don't wait for it to complete. The complexity of the implementation is a bit hard to derive, but it is roughly O(fib(n)). That is, its growth is proportional to the growth in the value of the result, and the growth rate of the Fibonacci sequence is substantial. For example, fib(120) is 8,670,007,398,507,948,658,051,921. If each recursive call took a nanosecond, fib(120) would take about 250,000 years to finish. Let's try and figure out why this implementation takes so long. Given the tiny amount of code in the body of fib, it's clear that the problem must be the number of times that fib calls itself. As an example, look at the tree of calls associated with the invocation fib(6). Figure 15-1 Tree of calls for recursive Fibonacci Notice that we are computing the same values over and over again. For example, fib gets called with 3 three times, and each of these calls provokes four additional calls of fib. It doesn't require a genius to think that it might be a good idea to record the value returned by the first call, and then look it up rather than compute it each time it is needed. This is the key idea behind dynamic programming. There are two approaches to dynamic programming Memoization solves the original problem top-down.

The variable is then assigned the second value in the sequence, and the code block is executed again. The process continues until the sequence is exhausted or a break statement is executed within the code block.

For example, the code total = 0 for num in (77, 11, 3): total = total + num print(total) will print 91. The expression (77, 11, 3) is a tuple. We discuss tuples in detail in Section 5. For now, just think of a tuple as a sequence of values. The sequence of values bound to variable is most commonly generated using the built-in function range, which returns a series of integers. The range function takes three integer arguments: start, stop, and step. It produces the progression start, start + step, start + 2*step, etc. If step is positive, the last element is the largest integer such that (start + i*step) is strictly less than stop. If step is negative, the last element is the smallest integer such that (start + i*step) is greater than stop. For example, the expression range(5, 40, 10) yields the sequence 5, 15, 25, 35, and the expression range(40, 5, -10) yields the sequence 40, 30, 20, 10. If the first argument to range is omitted, it defaults to 0, and if the last argument (the step size) is omitted, it defaults to 1. For example, range(0, 3) and range(3) both produce the sequence 0, 1, 2. The numbers in the progression are generated on an “as needed” basis, so even expressions such as range(1000000) consume little memory. We will discuss range in more depth in Section 5.2. Consider the code x = 4 for i in range(x): print(i) It prints 0 1 2 3 The code in Figure 2-9 reimplements the algorithm in Figure 2-7 for squaring an integer (corrected so that it works for negative numbers). Notice that unlike the while loop implementation, the number of iterations is not controlled by an explicit test, and the index variable num_iterations is not explicitly incremented. Figure 2-9 Using a for statement Notice that the code in Figure 2-9 does not change the value of num_iterations within the body of the for loop. This is typical, but not necessary, which raises the question of what happens if the index variable is modified within the for loop. Consider for i in range(2): print(i) i = 0 print(i) Do you think that it will print 0, 0, 1, 0, and then halt?

○ The check if node not in path prevents the program from getting caught in a cycle. ○ The check if shortest == None or len(path) < len(shortest) is used to decide if it is possible that continuing to search this path might yield a shorter path than the best path found so far. ○ If so, DFS is called recursively. If it finds a path to end that is no longer than the best found so far, shortest is updated. ○ When the last node on path has no children left to visit, the program backtracks to the previously visited node and visits the next child of that node. The function returns when all possible shortest paths from start to end have been explored. Figure 14-10 contains some code that runs the code in Figure 14- 9. The function test_SP in Figure 14-10 first builds a directed graph like the one pictured in the figure, and then searches for a shortest path between node 0 and node 5. Figure 14-10 Test depth-first-search code When executed, test_SP produces the output Current DFS path: 0 Current DFS path: 0->1 Current DFS path: 0->1->2 Current DFS path: 0->1->2->3 Current DFS path: 0->1->2->3->4 Current DFS path: 0->1->2->3->5 Current DFS path: 0->1->2->4 Current DFS path: 0->2 Current DFS path: 0->2->3 Current DFS path: 0->2->3->4 Current DFS path: 0->2->3->5 Current DFS path: 0->2->3->1 Current DFS path: 0->2->4 Shortest path found by DFS: 0->2->3->5 Notice that after exploring the path 0->1->2->3->4, it backs up to node 3 and explores the path 0->1->2->3->5. After saving that as the shortest successful path so far, it backs up to node 2 and explores the path 0->1->2->4.

It starts from the original problem, breaks it into subproblems, breaks the subproblems into subproblems, etc. Each time it solves a subproblem, it stores the answer in a table. Each time it needs to solve a subproblem, it first tries to look up the answer in the table. Tabular is a bottom-up method. It starts from the smallest problems, and stores the answers to those in a table. It then combines the solutions to these problems to solve the next smallest problems, and stores those answers in the table. Figure 15-2 contains implementations of Fibonacci using each approach to dynamic programming. The function fib_memo has a parameter, memo, that it uses to keep track of the numbers it has already evaluated. The parameter has a default value, the empty dictionary, so that clients of fib_memo don't have to worry about supplying an initial value for memo. When fib_memo is called with an n > 1, it attempts to look up n in memo. If it is not there (because this is the first time fib_memo has been called with that value), an exception is raised. When this happens, fib_memo uses the normal Fibonacci recurrence, and then stores the result in memo. The function fib_tab is quite simple. It exploits the fact that all of the subproblems for Fibonacci are known in advance and easy to enumerate in a useful order. Figure 15-2 Implementing Fibonacci using a memo If you try running fib_memo and fib_tab, you will see that they are indeed quite fast: fib(120) returns almost instantly. What is the complexity of these functions? fib_memo calls fib exactly once for each value from 0 to n. Therefore, under the assumption that dictionary lookup can be done in constant time, the time complexity of fib_memo(n) is in O(n).